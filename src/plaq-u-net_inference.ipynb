{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plaq-u-net: multi-patch consensus U-Net for automated detection and segmentation of the carotid arteries on black blood MRI sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Lavrova, 2022\n",
    "\n",
    "This is a code supporting the corresponding paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\E.Lavrova\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "import pydicom\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import cv2\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage import img_as_float\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from skimage import morphology\n",
    "from skimage.filters import threshold_otsu, threshold_local\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1115 20:39:58.012665 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1115 20:39:58.013662 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1115 20:39:58.015666 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W1115 20:40:00.620081 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'                        \n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "\n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_unet(input_img, n_filters = 8, dropout = 0.10, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1, training=True)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2, training=True)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3, training=True)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4, training=True)\n",
    "\n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6, training=True)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7, training=True)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8, training=True)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9, training=True)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models compilation + loading weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1115 20:40:00.871916 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1115 20:40:00.875916 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1115 20:40:00.876917 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W1115 20:40:00.903922 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1115 20:40:00.958929 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1115 20:40:01.069931 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1115 20:40:01.074932 48480 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1115 20:40:01.884310 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1115 20:40:02.959305 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W1115 20:40:04.087888 48480 module_wrapper.py:139] From C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "im_height = 64\n",
    "im_width = 64\n",
    "\n",
    "\n",
    "input_img = Input((im_height, im_width, 2), name='img')\n",
    "\n",
    "plaqunet_simple = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True)\n",
    "plaqunet_simple.load_weights('../res/plaq-u-net_simple_dce_2.h5')\n",
    "\n",
    "plaqunet_aug = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True)\n",
    "plaqunet_aug.load_weights('../res/plaq-u-net_aug_dce_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaquncertaintynet_simple = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True)\n",
    "plaquncertaintynet_simple.load_weights('../res/plaq-u-net_simple_ul_4.h5')\n",
    "\n",
    "plaquncertaintynet_aug = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True)\n",
    "plaquncertaintynet_aug.load_weights('../res/plaq-u-net_aug_ul_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 512\n",
    "IMG_SIDE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_img(img_arr):\n",
    "\n",
    "    img_min = np.min(img_arr) \n",
    "    img_max = np.max(img_arr) \n",
    "    \n",
    "    img_norm = np.copy((img_arr - img_min)/(img_max - img_min)*255).astype(np.uint8)\n",
    "        \n",
    "    return img_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_arrays(X, y, sub_names, dirname_img, dirname_nnunet, dirname_gt):\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    for sub_name in sub_names:\n",
    "       \n",
    "\n",
    "        filename_img = dirname_img + sub_name + '_0000.nii.gz'\n",
    "        filename_sm = dirname_nnunet + sub_name + '.npz'\n",
    "        filename_pkl = dirname_nnunet + sub_name + '.pkl'\n",
    "        filename_gt = dirname_gt + sub_name + '.nii.gz'\n",
    "\n",
    "        img = nib.load(filename_img).get_fdata().T\n",
    "        gt = nib.load(filename_gt).get_fdata().T\n",
    "        sm_cropped = np.load(filename_sm)['softmax']\n",
    "        with open(filename_pkl, 'rb') as f:\n",
    "            p = pickle.load(f)\n",
    "        crop_box = p['crop_bbox']\n",
    "        sm = np.zeros(img.shape, dtype=np.float16)\n",
    "        sm[crop_box[0][0]:crop_box[0][1], crop_box[1][0]:crop_box[1][1], crop_box[2][0]:crop_box[2][1]] = sm_cropped[1, ...]\n",
    "\n",
    "        for j in range (0, img.shape[0]):\n",
    "            img_norm = norm_img(img[j, ...])\n",
    "            X[i, ..., 0] = img_norm\n",
    "            X[i, ..., 1] = 255*sm[j, ...]\n",
    "            y[i, ..., 0] = gt[j, ...]\n",
    "            i += 1\n",
    "                \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect4multipatches_0(img, model):\n",
    "    \n",
    "    img_padded = np.zeros((IMG_HEIGHT+128, IMG_WIDTH+128, 2), dtype=np.uint8)\n",
    "    img_padded[64:-64, 64:-64, :] = img\n",
    "    \n",
    "    sm = img_padded[..., 1]\n",
    "    sm_bin = (sm>0).astype(np.uint8)\n",
    "    sm_bin_label = label(sm_bin)\n",
    "    label_weights = []\n",
    "    for l in range(1, np.max(sm_bin_label)+1):\n",
    "        mask_label = (sm_bin_label==l).astype(np.uint8)\n",
    "        weight_label = np.sum(mask_label*sm)\n",
    "        rec_lw = {'label': l, 'weight': weight_label}\n",
    "        label_weights.append(rec_lw)\n",
    "    label_weights = pd.DataFrame(label_weights)\n",
    "    n_labels = min(len(label_weights), 1)\n",
    "    labels = []\n",
    "    if len(label_weights)>0:\n",
    "        label_weights.sort_values(by='weight', inplace=True, ascending = False)\n",
    "        labels = np.array(label_weights['label'])[:n_labels]\n",
    "    contour = np.isin(sm_bin_label, labels).astype(np.uint8)\n",
    "    contour_pixels = np.where(contour>0)\n",
    "    pred_padded = np.zeros((img_padded.shape[0], img_padded.shape[1]))\n",
    "    if np.sum(contour)>0:\n",
    "        x_center = int(np.mean(contour_pixels[0]))\n",
    "        y_center = int(np.mean(contour_pixels[1]))\n",
    "\n",
    "        img_patch = np.zeros((1, 64, 64, 2))\n",
    "        img_patch[0, ...] = img_padded[x_center-32:x_center+32, y_center-32:y_center+32, :].copy()\n",
    "\n",
    "        img_patch_pred = model.predict(img_patch, verbose=0)\n",
    "        pred_padded[x_center-32:x_center+32, y_center-32:y_center+32] = img_patch_pred[..., 0]\n",
    "            \n",
    "    return pred_padded[64:-64, 64:-64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CA probability maps calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_array(sub_name, dirname_img, dirname_nnunet):\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    filename_img = dirname_img + sub_name + '_0000.nii.gz'\n",
    "    filename_sm = dirname_nnunet + sub_name + '.npz'\n",
    "    filename_pkl = dirname_nnunet + sub_name + '.pkl'\n",
    "    img_nii = nib.load(filename_img)\n",
    "    img = img_nii.get_fdata().T\n",
    "    sm_cropped = np.load(filename_sm)['softmax']\n",
    "    with open(filename_pkl, 'rb') as f:\n",
    "        p = pickle.load(f)\n",
    "    crop_box = p['crop_bbox']\n",
    "    sm = np.zeros(img.shape, dtype=np.float16)\n",
    "    sm[crop_box[0][0]:crop_box[0][1], crop_box[1][0]:crop_box[1][1], crop_box[2][0]:crop_box[2][1]] = sm_cropped[1, ...]\n",
    "    \n",
    "    X = np.zeros((img.shape[0], img.shape[1], img.shape[2], 2), dtype = np.uint8)\n",
    "\n",
    "    for j in range (0, img.shape[0]):\n",
    "        img_norm = norm_img(img[j, ...])\n",
    "        X[i, ..., 0] = img_norm\n",
    "        X[i, ..., 1] = 255*sm[j, ...]\n",
    "        i += 1\n",
    "              \n",
    "    return X, img_nii.affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_names_test = ['AMC012', 'AMC006', \n",
    "                  'MUMC094', 'MUMC027', 'MUMC079', 'MUMC052', 'MUMC127', 'MUMC071', 'MUMC038', 'MUMC093', 'MUMC107', \n",
    "                  'MUMC022', 'MUMC114', 'MUMC115', 'MUMC069', 'MUMC130', 'MUMC036', 'MUMC007', 'MUMC059', 'MUMC080', \n",
    "                  'UMCU036', 'UMCU025', 'UMCU008', 'UMCU034']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_names_emc = ['EMC003', 'EMC004', 'EMC005', 'EMC007', 'EMC008', 'EMC009', 'EMC011', \n",
    "                 'EMC015', 'EMC018', 'EMC020', 'EMC024', 'EMC027', 'EMC029', 'EMC031', \n",
    "                 'EMC032', 'EMC034', 'EMC035', 'EMC036', 'EMC038', 'EMC041', 'EMC042', \n",
    "                 'EMC043', 'EMC045', 'EMC046', 'EMC047', 'EMC048', 'EMC049', 'EMC050', \n",
    "                 'EMC051', 'EMC052', 'EMC054', 'EMC055', 'EMC056', 'EMC057']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname_imgdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/imagesTs1/'\n",
    "dirname_gtdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/labelsTs1/'\n",
    "dirname_nnunetdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_results/Ts1/'\n",
    "dirname_results_test = '../res/nifti_compare/test_plaqunet_epochs/'\n",
    "dirname_results_test_p = '../res/nifti_compare/test_plaqunet_epochs_p/'\n",
    "\n",
    "for sub_name in sub_names_test:\n",
    "    X, affine_nii = fill_array(sub_name, dirname_imgdata_test, dirname_nnunetdata_test)\n",
    "    pred = np.zeros((X.shape[0], X.shape[1], X.shape[2]), dtype=np.float32)\n",
    "    for i in range(0, X.shape[0]):\n",
    "        pred_slice = detect4multipatches_0(X[i, ...], plaqunet_aug)\n",
    "        pred[i, ...] = (pred_slice).astype(np.float32)\n",
    "    nifti_pred = nib.Nifti1Image((pred>0.5).astype(np.uint8).T, affine=affine_nii)\n",
    "    nifti_pred_p = nib.Nifti1Image(pred.T, affine=affine_nii)\n",
    "    nib.save(nifti_pred, dirname_results_test + sub_name + '.nii.gz')\n",
    "    nib.save(nifti_pred_p, dirname_results_test_p + sub_name + '.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dirname_imgdata_t2w = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/imagesTs2/'\n",
    "dirname_gtdata_t2w = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/labelsTs2/'\n",
    "dirname_nnunetdata_t2w = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_results/Ts2/'\n",
    "dirname_results_t2w = '../res/nifti_compare/t2w_plaqunet_epochs/'\n",
    "dirname_results_t2w_p = '../res/nifti_compare/t2w_plaqunet_epochs_p/'\n",
    "\n",
    "for sub_name in sub_names_test:\n",
    "    X, affine_nii = fill_array(sub_name, dirname_imgdata_t2w, dirname_nnunetdata_t2w)\n",
    "    pred = np.zeros((X.shape[0], X.shape[1], X.shape[2]), dtype=np.float32)\n",
    "    for i in range(0, X.shape[0]):\n",
    "        pred_slice = detect4multipatches_0(X[i, ...], plaqunet_aug)\n",
    "        pred[i, ...] = (pred_slice).astype(np.float32)\n",
    "    nifti_pred = nib.Nifti1Image((pred>0.5).astype(np.uint8).T, affine=affine_nii)\n",
    "    nifti_pred_p = nib.Nifti1Image(pred.T, affine=affine_nii)\n",
    "    nib.save(nifti_pred, dirname_results_t2w + sub_name + '.nii.gz')\n",
    "    nib.save(nifti_pred_p, dirname_results_t2w_p + sub_name + '.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dirname_imgdata_t1wce = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/imagesTs3/'\n",
    "dirname_gtdata_t1wce = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/labelsTs3/'\n",
    "dirname_nnunetdata_t1wce = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_results/Ts3/'\n",
    "dirname_results_t1wce = '../res/nifti_compare/t1wce_plaqunet_epochs/'\n",
    "dirname_results_t1wce_p = '../res/nifti_compare/t1wce_plaqunet_epochs_p/'\n",
    "\n",
    "for sub_name in sub_names_test:\n",
    "    X, affine_nii = fill_array(sub_name, dirname_imgdata_t1wce, dirname_nnunetdata_t1wce)\n",
    "    pred = np.zeros((X.shape[0], X.shape[1], X.shape[2]), dtype=np.float32)\n",
    "    for i in range(0, X.shape[0]):\n",
    "        pred_slice = detect4multipatches_0(X[i, ...], plaqunet_aug)\n",
    "        pred[i, ...] = (pred_slice).astype(np.float32)\n",
    "    nifti_pred = nib.Nifti1Image((pred>0.5).astype(np.uint8).T, affine=affine_nii)\n",
    "    nifti_pred_p = nib.Nifti1Image(pred.T, affine=affine_nii)\n",
    "    nib.save(nifti_pred, dirname_results_t1wce + sub_name + '.nii.gz')\n",
    "    nib.save(nifti_pred_p, dirname_results_t1wce_p + sub_name + '.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname_imgdata_emc = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/imagesTs4/'\n",
    "dirname_gtdata_emc = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/labelsTs4/'\n",
    "dirname_nnunetdata_emc = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_results/Ts4/'\n",
    "dirname_results_emc = '../res/nifti_compare/emc_plaqunet_epochs/'\n",
    "dirname_results_emc_p = '../res/nifti_compare/emc_plaqunet_epochs_p/'\n",
    "\n",
    "for sub_name in sub_names_emc:\n",
    "    X, affine_nii = fill_array(sub_name, dirname_imgdata_emc, dirname_nnunetdata_emc)\n",
    "    pred = np.zeros((X.shape[0], X.shape[1], X.shape[2]), dtype=np.float32)\n",
    "    for i in range(0, X.shape[0]):\n",
    "        pred_slice = detect4multipatches_0(X[i, ...], plaqunet_aug)\n",
    "        pred[i, ...] = (pred_slice).astype(np.float32)\n",
    "    nifti_pred = nib.Nifti1Image((pred>0.5).astype(np.uint8).T, affine=affine_nii)\n",
    "    nifti_pred_p = nib.Nifti1Image(pred.T, affine=affine_nii)\n",
    "    nib.save(nifti_pred, dirname_results_emc + sub_name + '.nii.gz')\n",
    "    nib.save(nifti_pred_p, dirname_results_emc_p + sub_name + '.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname_imgdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/imagesTs1/'\n",
    "dirname_gtdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/labelsTs1/'\n",
    "dirname_nnunetdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_results/Ts1/'\n",
    "dirname_results_test = '../res/nifti_compare/test_plaqumap_dropout/'\n",
    "\n",
    "for sub_name in sub_names_test:\n",
    "    X, affine_nii = fill_array(sub_name, dirname_imgdata_test, dirname_nnunetdata_test)\n",
    "    for j in range (0, 10):\n",
    "        pred = np.zeros((X.shape[0], X.shape[1], X.shape[2]), dtype=np.float32)\n",
    "        for i in range(0, X.shape[0]):\n",
    "            pred_slice = detect4multipatches_0(X[i, ...], plaquncertaintynet_aug)\n",
    "            pred[i, ...] = (pred_slice).astype(np.float32)\n",
    "        nifti_pred = nib.Nifti1Image(pred.T, affine=affine_nii)\n",
    "        nib.save(nifti_pred, dirname_results_test + sub_name + '_' + str(j) + '.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dirname_imgdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/imagesTs2/'\n",
    "dirname_gtdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/labelsTs2/'\n",
    "dirname_nnunetdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_results/Ts2/'\n",
    "dirname_results_test = '../res/nifti_compare/t2w_plaqumap_dropout/'\n",
    "\n",
    "for sub_name in sub_names_test:\n",
    "    X, affine_nii = fill_array(sub_name, dirname_imgdata_test, dirname_nnunetdata_test)\n",
    "    for j in range (0, 10):\n",
    "        pred = np.zeros((X.shape[0], X.shape[1], X.shape[2]), dtype=np.float32)\n",
    "        for i in range(0, X.shape[0]):\n",
    "            pred_slice = detect4multipatches_0(X[i, ...], plaquncertaintynet_aug)\n",
    "            pred[i, ...] = (pred_slice).astype(np.float32)\n",
    "        nifti_pred = nib.Nifti1Image(pred.T, affine=affine_nii)\n",
    "        nib.save(nifti_pred, dirname_results_test + sub_name + '_' + str(j) + '.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\segway\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dirname_imgdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/imagesTs3/'\n",
    "dirname_gtdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/labelsTs3/'\n",
    "dirname_nnunetdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_results/Ts3/'\n",
    "dirname_results_test = '../res/nifti_compare/t1wce_plaqumap_dropout/'\n",
    "\n",
    "for sub_name in sub_names_test:\n",
    "    X, affine_nii = fill_array(sub_name, dirname_imgdata_test, dirname_nnunetdata_test)\n",
    "    for j in range (0, 10):\n",
    "        pred = np.zeros((X.shape[0], X.shape[1], X.shape[2]), dtype=np.float32)\n",
    "        for i in range(0, X.shape[0]):\n",
    "            pred_slice = detect4multipatches_0(X[i, ...], plaquncertaintynet_aug)\n",
    "            pred[i, ...] = (pred_slice).astype(np.float32)\n",
    "        nifti_pred = nib.Nifti1Image(pred.T, affine=affine_nii)\n",
    "        nib.save(nifti_pred, dirname_results_test + sub_name + '_' + str(j) + '.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname_imgdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/imagesTs4/'\n",
    "dirname_gtdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_raw_data/Task001_CA/labelsTs4/'\n",
    "dirname_nnunetdata_test = 'D:/Lisa/nnUNet_raw_data_base/nnUNet_results/Ts4/'\n",
    "dirname_results_test = '../res/nifti_compare/emc_plaqumap_dropout/'\n",
    "\n",
    "for sub_name in sub_names_emc:\n",
    "    X, affine_nii = fill_array(sub_name, dirname_imgdata_test, dirname_nnunetdata_test)\n",
    "    for j in range (0, 10):\n",
    "        pred = np.zeros((X.shape[0], X.shape[1], X.shape[2]), dtype=np.float32)\n",
    "        for i in range(0, X.shape[0]):\n",
    "            pred_slice = detect4multipatches_0(X[i, ...], plaquncertaintynet_aug)\n",
    "            pred[i, ...] = (pred_slice).astype(np.float32)\n",
    "        nifti_pred = nib.Nifti1Image(pred.T, affine=affine_nii)\n",
    "        nib.save(nifti_pred, dirname_results_test + sub_name + '_' + str(j) + '.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
